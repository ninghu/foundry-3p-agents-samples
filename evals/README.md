# Trace Evaluation

Utilities in this folder show how to pull trace IDs from Azure Application Insights and score them with Azure AI Project evaluators. The centerpiece is `trace_eval.py`, a fully scripted flow that:
1. Loads configuration from `.env`.
2. Queries Application Insights for traces generated by a specific agent ID.
3. Creates an evaluation group with built-in Azure AI evaluators.
4. Submits an evaluation run backed by the collected trace IDs.
5. Polls the run until it finishes and prints the results.

## Prerequisites
- Azure Application Insights instance that captures your agent traces. Grant the project’s managed identity (or whatever identity `DefaultAzureCredential` uses) the **Log Analytics Reader** role on this resource.
- Azure AI Project endpoint URL (e.g., `https://<region>.services.ai.azure.com/api/projects/<projectName>`).
- Ability to authenticate via `DefaultAzureCredential` (Azure CLI login, managed identity, or service principal environment variables).
- Python 3.12+ and the dependencies listed in `evals/requirements.txt`.

## Configure environment variables
Create `evals/.env` using the following keys:

```env
# Resource ID of the Application Insights instance (starts with /subscriptions/...)
APPINSIGHTS_RESOURCE_ID=/subscriptions/<sub>/resourceGroups/<rg>/providers/microsoft.insights/components/<name>

# Azure AI Project endpoint (include the /api/projects/... suffix)
PROJECT_ENDPOINT=https://your-project.services.ai.azure.com/api/projects/your-project
```

`trace_eval.py` also contains two sane defaults you can override if needed:
- `agent_id = "gcp-cloud-run-agent"` – change this if you want to evaluate another agent’s traces.
- `_build_evaluator_config(... deployment_name="ninhuaiswedencentral/gpt-35-turbo")` – swap in whatever Azure OpenAI deployment should power the Task Adherence / Intent Resolution evaluators.

## Running the script
```bash
cd evals
pip install -r requirements.txt
python trace_eval.py
```
The script uses `DefaultAzureCredential`, so make sure you are logged in (`az login`) or have the appropriate environment variables set before running it.

Sample output includes:
- Query progress + number of trace IDs returned from Application Insights.
- Evaluation group metadata as returned by the Azure AI Projects client.
- Evaluation run status updates until the run is `completed`, `failed`, or `canceled`.

## Customising the workflow
- Adjust `start_time` / `end_time` in `trace_eval.py` if you want to scan a wider/smaller time window.
- Add or remove evaluators by editing the `_build_evaluator_config` list.
- Supply explicit `trace_ids` if you already have them (skip the query step and jump straight to the evaluation run).

For a lighter-weight approach, you can reimplement the steps manually using the REST API (`/openai/evals` and `/openai/evals/{evalId}/runs`) – the script mirrors those calls via the Python SDK.
