# Evaluation Utilities

Utilities in this folder show multiple ways to score agents with Azure AI Evaluations. Two ready-to-run flows are included:
- `trace/trace_eval.py` pulls trace IDs from Azure Application Insights and submits them to an Azure AI Project evaluation run.
- `a2a/a2a_agent_eval.py` drives the remote currency agent over the A2A protocol, captures responses, and scores them locally before optionally pushing the results to your Azure AI Project.

## Trace Evaluation (`trace/trace_eval.py`)

`trace_eval.py` is a fully scripted flow that:
1. Loads configuration from `trace/.env`.
2. Queries Application Insights for traces generated by a specific agent ID.
3. Creates an evaluation group with built-in Azure AI evaluators.
4. Submits an evaluation run backed by the collected trace IDs.
5. Polls the run until it finishes and prints the results.

### Prerequisites
- Azure Application Insights instance that captures your agent traces. Grant the project’s managed identity (or whatever identity `DefaultAzureCredential` uses) the **Log Analytics Reader** role on this resource.
- Azure AI Project endpoint URL (e.g., `https://<region>.services.ai.azure.com/api/projects/<projectName>`).
- Ability to authenticate via `DefaultAzureCredential` (Azure CLI login, managed identity, or service principal environment variables).
- Python 3.12+ and the dependencies listed in `evals/trace/requirements.txt`.

### Configure environment variables
Create `evals/trace/.env` using the following keys:

```env
# Resource ID of the Application Insights instance (starts with /subscriptions/...)
APPINSIGHTS_RESOURCE_ID=/subscriptions/<sub>/resourceGroups/<rg>/providers/microsoft.insights/components/<name>

# Azure AI Project endpoint (include the /api/projects/... suffix)
PROJECT_ENDPOINT=https://your-project.services.ai.azure.com/api/projects/your-project
```

`trace_eval.py` also contains two sane defaults you can override if needed:
- `agent_id = "gcp-cloud-run-agent"` – change this if you want to evaluate another agent’s traces.
- `_build_evaluator_config(... deployment_name="ninhuaiswedencentral/gpt-35-turbo")` – swap in whatever Azure OpenAI deployment should power the Task Adherence / Intent Resolution evaluators.

### Running the script
```bash
cd evals/trace
pip install -r requirements.txt
python trace_eval.py
```
The script uses `DefaultAzureCredential`, so make sure you are logged in (`az login`) or have the appropriate environment variables set before running it.

Sample output includes:
- Query progress + number of trace IDs returned from Application Insights.
- Evaluation group metadata as returned by the Azure AI Projects client.
- Evaluation run status updates until the run is `completed`, `failed`, or `canceled`.

### Customising the workflow
- Adjust `start_time` / `end_time` in `trace_eval.py` if you want to scan a wider/smaller time window.
- Add or remove evaluators by editing the `_build_evaluator_config` list.
- Supply explicit `trace_ids` if you already have them (skip the query step and jump straight to the evaluation run).

For a lighter-weight approach, you can reimplement the steps manually using the REST API (`/openai/evals` and `/openai/evals/{evalId}/runs`) – the script mirrors those calls via the Python SDK.

## Remote A2A Agent Evaluation (`a2a/a2a_agent_eval.py`)

The `a2a` subfolder bundles everything needed to drive the remote currency agent (now located under `gcp/a2a_agent`) through an Azure AI connection and score each answer with the Task Adherence and Intent Resolution evaluators.

### Prerequisites
- A deployed remote agent accessible via A2A (for example the sample in `gcp/a2a_agent`).
- Azure AI Project connection that points to that agent. By default the script looks for a connection named `gcpa2a` (see the `agent_id` constant near the bottom of the script).
- Azure OpenAI deployment for evaluator calls (`TaskAdherenceEvaluator` + `IntentResolutionEvaluator`).
- Authentication through `DefaultAzureCredential` so the script can fetch the connection metadata from your Azure AI Project.
- Python 3.12+ and the dependencies listed in `evals/a2a/requirements.txt`.

### Configure environment variables
```bash
cd evals/a2a
cp .env.example .env
```
Set the following keys in `.env`:
- `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_DEPLOYMENT`, `AZURE_OPENAI_KEY` – power the evaluator models.
- `AZURE_AI_PROJECT_ENDPOINT` – the `/api/projects/...` URL used for both evaluator result uploads and to fetch the remote agent connection/credentials.

Feel free to edit `dataset.jsonl` to supply your own prompts/responses; the helper `_create_dataset_with_agent_id` will inject the `agent_id` column required by the Azure AI evaluation APIs.

### Run the evaluation
```bash
cd evals/a2a
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\Activate.ps1
pip install -r requirements.txt
python a2a_agent_eval.py
```
The script reads credentials from `.env`, grabs the Azure AI connection for the configured `agent_id`, invokes the remote agent for every dataset row, and prints the evaluator scores. Results are also uploaded to the Azure AI Project specified in `.env`, and the temporary dataset created during the run is cleaned up automatically.
